{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d02025c1f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.reinforce_trainer import ReinforceTrainer\n",
    "from src.reinforce_config import ReinforceConfig\n",
    "from src.reinforce_trainer import generate, score_sequence\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Юрий\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lvwerra/gpt2-imdb\"\n",
    "policy = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "ref_policy = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\n",
    "reward_model_name = \"models/reward/checkpoint-391\"\n",
    "# reward_model_name = \"lvwerra/distilbert-imdb\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    prompt_len = random.randint(5, 15)\n",
    "    examples[\"input_ids\"] = policy_tokenizer.encode(examples[\"text\"])[:prompt_len]\n",
    "    examples[\"query\"] = policy_tokenizer.decode(examples[\"input_ids\"])\n",
    "    return examples\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "prompt_dataset = dataset.map(tokenize, batched=False)\n",
    "\n",
    "test_dataset = prompt_dataset[\"test\"].select(range(100))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset.remove_columns([\"text\", \"label\", \"query\"]),\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=DataCollatorWithPadding(policy_tokenizer),\n",
    ")\n",
    "\n",
    "train_dataset = prompt_dataset[\"train\"].remove_columns([\"text\", \"label\", \"query\"])\n",
    "eval_dataset = prompt_dataset[\"test\"].remove_columns([\"text\", \"label\", \"query\"])\n",
    "eval_dataset = eval_dataset.train_test_split(0.1, seed=1)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = ReinforceConfig(\n",
    "    output_dir=\"_\",\n",
    "\n",
    "    per_device_train_batch_size=64,\n",
    "    local_rollout_forward_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.0,\n",
    "    seed=1,\n",
    "\n",
    "    rloo_k=1,\n",
    "    num_ppo_epochs=1,\n",
    "    num_mini_batches=1,\n",
    "    ema_beta=0.99,\n",
    "    kl_coef=1.0,\n",
    "    temperature=0.7,\n",
    ")\n",
    "training_args.total_episodes = 400 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.get_reward import BertRewardGetter\n",
    "\n",
    "trainer = ReinforceTrainer(\n",
    "    training_args,\n",
    "    tokenizer=policy_tokenizer,\n",
    "    policy=policy,\n",
    "    ref_policy=ref_policy,\n",
    "    reward_model=reward_model,\n",
    "    get_reward_func=BertRewardGetter(policy_tokenizer, reward_tokenizer).get_reward,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = int(time())\n",
    "trainer.args.output_dir = f\"models/reinforce/{timestamp}\"\n",
    "save_path = Path(trainer.args.output_dir)\n",
    "trainer.train()\n",
    "\n",
    "policy.save_pretrained(save_path / \"policy\", from_pt=True)\n",
    "ref_policy.save_pretrained(save_path / \"ref_policy\", from_pt=True)\n",
    "trainer.save_state()\n",
    "with open(save_path / \"args.json\") as f:\n",
    "    f.write(trainer.args.to_json_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_ops import model_add_, model_sub_, model_lerp_, model_slerp_\n",
    "\n",
    "training_args.per_device_train_batch_size = 64\n",
    "training_args.total_episodes = 100 * training_args.per_device_train_batch_size\n",
    "training_args.ema_beta = 0.99\n",
    "training_args.iterations = 2\n",
    "training_args.runs = 2\n",
    "training_args.eta = 0.5\n",
    "\n",
    "policy_init = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "timestamp = int(time())\n",
    "\n",
    "for i in range(training_args.iterations):\n",
    "    for m in range(training_args.runs):\n",
    "        # initialize policies\n",
    "        policy_m = deepcopy(policy_init)\n",
    "        policy_ref_m = deepcopy(policy_init)\n",
    "\n",
    "        # perform training\n",
    "        train_dataset = train_dataset.shuffle(seed=1)\n",
    "        trainer = ReinforceTrainer(\n",
    "            training_args,\n",
    "            tokenizer=policy_tokenizer,\n",
    "            policy=policy_m,\n",
    "            ref_policy=policy_ref_m,\n",
    "            reward_model=reward_model,\n",
    "            get_reward_func=BertRewardGetter(policy_tokenizer, reward_tokenizer).get_reward,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset\n",
    "        )\n",
    "        trainer.args.output_dir = f\"models/warp/{timestamp}/i{i+1}m{m+1}\"\n",
    "        trainer.train()\n",
    "        policy_m.save_pretrained(Path(trainer.args.output_dir) / \"policy\", from_pt=True)\n",
    "        policy_ref_m.save_pretrained(Path(trainer.args.output_dir) / \"ref_policy\", from_pt=True)\n",
    "        trainer.save_state()\n",
    "\n",
    "        # compute task vector inplace\n",
    "        model_sub_(policy_m, policy_init)\n",
    "        # collect slerp iterations inplace\n",
    "        if m == 0:\n",
    "            policy_slerp = policy_m\n",
    "        else:\n",
    "            model_slerp_(policy_slerp, policy_m, coeff=1/training_args.runs)\n",
    "    # add init back to complete slerp\n",
    "    model_add_(policy_slerp, policy_init)\n",
    "    # update init inplace\n",
    "    model_lerp_(policy_init, policy_slerp, coeff=training_args.eta)\n",
    "\n",
    "with open(f\"models/warp/{timestamp}/args.json\") as f:\n",
    "    f.write(trainer.args.to_json_string())\n",
    "policy_init.save_pretrained(f\"models/warp/{timestamp}/final/policy\", from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, ref_policy, test_dataloader):\n",
    "    rewards = 0\n",
    "    kls = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            prompts = data[\"input_ids\"].to(policy.device)\n",
    "            prompt_length = prompts.shape[1]\n",
    "            prompts_responses, _ = generate(\n",
    "                policy,\n",
    "                prompts,\n",
    "                policy_tokenizer.pad_token_id,\n",
    "                trainer.generation_config,\n",
    "            )\n",
    "            logprobs = score_sequence(\n",
    "                policy,\n",
    "                prompts_responses,\n",
    "                prompt_length,\n",
    "                pad_token_id=policy_tokenizer.pad_token_id,\n",
    "                temperature=training_args.temperature\n",
    "            )\n",
    "            ref_logprobs = score_sequence(\n",
    "                ref_policy,\n",
    "                prompts_responses,\n",
    "                prompt_length,\n",
    "                pad_token_id=policy_tokenizer.pad_token_id,\n",
    "                temperature=training_args.temperature\n",
    "            )\n",
    "            _, reward, _ = trainer.get_reward(\n",
    "                reward_model, prompts_responses, policy_tokenizer.pad_token_id, prompt_length\n",
    "            )\n",
    "            kl = (logprobs.exp() * (logprobs - ref_logprobs)).sum(1)\n",
    "\n",
    "            rewards += reward.sum().item()\n",
    "            kls += kl.sum().item()\n",
    "            total += prompts.shape[0]\n",
    "    rewards /= total\n",
    "    kls /= total\n",
    "    return rewards, kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = AutoModelForCausalLM.from_pretrained(\"models/warp/1722875607/final/policy\").cuda()\n",
    "ref_poicy = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "rs, kls = 0, 0\n",
    "for _ in range(10):\n",
    "    r, kl = evaluate_policy(policy, ref_policy, test_dataloader)\n",
    "    rs += r\n",
    "    kls += kl\n",
    "rs /= 10\n",
    "kls /= 10\n",
    "print(rs, kls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
